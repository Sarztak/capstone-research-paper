% Chapter: Introduction
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\section{Introduction}

\subsection{Problem Statement}

Drone light shows represent a unique synthesis of creative design and algorithmic precision, transforming abstract artistic concepts into tightly coordinated aerial performances. While their visual and cultural impact has grown rapidly, the process of designing such shows remains complex, highly specialized, and constrained by strict physical and regulatory boundaries. Translating human imagination into executable drone trajectories requires both aesthetic sensitivity and formal guarantees of safety, spacing, and feasibility.

Recent proposals have explored using Large Language Models (LLMs) to bridge the gap between natural language expression and swarm behavior generation. In principle, this approach offers an intuitive design interface: a user describes the desired effect --- “form a spiral that blossoms into a sphere” --- and an LLM translates that prompt into flight paths or formation parameters. However, existing systems such as CLIPSwarm, SwarmGPT, and LLM-Flock have largely remained proof-of-concept demonstrations. These architectures use language models as semantic translators, producing intermediate representations such as geometric outlines or waypoints, which are then refined through separate analytic solvers. As a result, the LLM occupies a peripheral role: it initiates the design process but does not ensure executable, safe, or optimized outcomes.

The practical consequence of this separation is that current research prototypes have not achieved the level of reliability or workflow integration required for real-world deployment. The artistic and semantic potential of language-based co-design remains underutilized, while the technical constraints that define feasible drone motion are handled independently through manual or traditional optimization methods. This disconnect mirrors broader challenges in LLM-assisted content generation, where models excel at producing semantically rich ideas but struggle when constrained by formal syntactic or physical rules.

Our work begins from this recognition and seeks to explore whether these two modalities — the semantic flexibility of language models and the syntactic precision of analytical systems — can be combined more effectively. Specifically, we propose an integrated pipeline in which the LLM functions not as a direct controller but as a generative front-end for a structured, verifiable production system. The final stages of this pipeline, including trajectory validation and compilation into deployable show files, are handled through the \textbf{Skybrush Studio API}, a professional toolchain for drone show design and execution. This framework grounds the generative capabilities of LLMs within the constraints of industry-standard safety and performance requirements.

\subsection{Analysis Goals}

The goal of this research is to evaluate how generative systems can contribute to the early design stages of drone light shows by integrating semantic input with deterministic analytical workflows. Rather than demonstrating fully autonomous choreography, the project examines how image-based generation and lightweight prompting can be coupled to existing validation tools to produce formations that are both expressive and executable.

Our analysis focuses on four questions:

\begin{enumerate}
  \item What constraints shape the current workflow of professional drone show design, and where does early-stage creative exploration slow the process?

  \item How can generative tools—either through images or lightweight language prompting—provide meaningful starting points for formation design?

  \item How can sampling and analytical validation ensure that these generated formations are transformed into physically feasible, drone-ready outputs?

  \item What role can production systems such as Skybrush Studio play in grounding generative content within real operational constraints?
\end{enumerate}

This work treats generative models not as autonomous designers but as one component in a broader pipeline. The study examines how their outputs can be mediated, constrained, and ultimately rendered useful when paired with established analytical solvers.

\subsection{Scope}

This project is limited to the design phase of drone light show development. The focus is on transforming conceptual visual inputs—user-provided images or lightweight generative prompts—into validated drone formations. The work does not involve flight testing, real-time control, or new control algorithms. Instead, it concentrates on bridging creative generation with analytic feasibility.

The scope includes:

\begin{itemize}
  \item Implementing a modular pipeline: image acquisition, 2D point sampling, analytic validation, and Skybrush-based compilation.
  \item Evaluating sampling strategies for converting images into drone-ready formations.
  \item Integrating the pipeline with the Skybrush Studio API for validation and deployment compatibility.
  \item Conducting conceptual exploration of how such a pipeline could be extended toward MCP-style interaction or 3D formation generation.
\end{itemize}

The project does not aim to create a standalone generative model for drone trajectories or a fully automated end-to-end design system. Instead, it formalizes and demonstrates a constrained but functional workflow that connects generative creativity with established, safety-pragmatic analytic tools.


