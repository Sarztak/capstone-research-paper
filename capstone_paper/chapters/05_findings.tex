% Chapter: Findings
\section{Findings}

This project does not aim to demonstrate autonomous LLM-driven choreography, but rather to assess the feasibility of a constrained pipeline that links generative visual input to analytically validated drone formations. The findings presented here reflect the empirical performance of the implemented system and the practical constraints observed during its construction.

\subsection{1. Sampling Quality and Formation Fidelity}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/findings/wabbit_grid.png}
        \caption*{Grid Sampling}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/findings/wabbit_blue.png}
        \caption*{Blue-Noise Sampling}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/findings/wabbit_farthest.png}
        \caption*{Farthest-Point Sampling}
    \end{subfigure}

    \vspace{4mm}

    \caption{\textbf{Comparison of sampling strategies used to convert binary silhouettes into drone formations.}
    Grid sampling runs efficiently but produces uneven density; blue-noise sampling yields scattered and harder-to-recognize forms; 
    farthest-point sampling preserves global structure while maintaining minimum inter-drone spacing.}
\end{figure}

Evaluation of the image-to-formation stage showed that the choice of sampling strategy has a decisive impact on the recognizability and structural coherence of the resulting drone layouts. As illustrated in Figure 2, grid sampling runs efficiently but produces uneven point density that distorts fine details. Blue-noise sampling generates more uniformly spaced points, yet the result appears visually scattered and often loses the silhouette’s global structure. Farthest-point sampling consistently produced the most stable and interpretable formations by preserving overall shape while maintaining minimum spacing between drones, making it the most suitable method for converting binary silhouettes into deployable formations.valuation of the image-to-formation stage demonstrated that sampling strategy plays a decisive role in the recognizability and structural coherence of the resulting drone layouts. Grid sampling proved efficient but produced uneven density. Blue-noise sampling yielded visually scattered formations. Farthest-point sampling consistently preserved global structure while maintaining minimum separation constraints, making it the most suitable method for converting silhouettes into stable drone formations.

\subsection{2. Integration Feasibility with Skybrush Studio}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[height=4.5cm]{figures/findings/takeoff_grid.png}
        \caption{Takeoff grid}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[height=4.5cm]{figures/findings/ascent_transition.png}
        \caption{Transition phase}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[height=4.5cm]{figures/findings/final_formation.png}
        \caption{Final sampled formation}
    \end{subfigure}
    \caption{Rendered frames from the compiled \texttt{.skyc} file inside Skybrush Studio, showing the takeoff sequence, transition motion, and final assembled formation.}
    \label{fig:skyc_render}
\end{figure}



The pipeline successfully exported the sampled coordinates as a \texttt{.csv} file and compiled them into a \texttt{.skyc} show file using Skybrush Studio. Rendering the resulting show in the
Skybrush simulator confirmed that the generated formation data were structurally consistent and
interpreted correctly by the production-grade toolchain. The animation demonstrated that the
drones lifted from a standard takeoff grid, followed interpolated transition paths, and assembled
into the sampled formation without inconsistencies in spacing or trajectory alignment.


\subsection{3. Constraints of 2D Static Formations}

Because the implemented system focuses on static 2D silhouettes, it does not address volumetric formations, temporal transitions, or dynamic choreography. This constraint clarified the limits of using images as semantic inputs: while effective for capturing shapes, they do not encode motion, timing, or 3D structure. These limitations reinforce the need for additional stages if such a pipeline were extended toward full performance design.

\subsection{4. Practical Insights for Pipeline Design}

The development process yielded several practical insights regarding modular system design. First, separating semantic input from analytical validation reduces failure modes when integrating heterogeneous tools. Second, using established solvers such as Skybrush Studio for constraint enforcement ensures that prototype systems remain compatible with real production infrastructures. Third, even limited generative input—such as silhouettes—can meaningfully accelerate early-stage formation design when paired with deterministic post-processing.

\subsection{5. Summary}

Overall, the findings indicate that a constrained, image-driven pipeline can produce analytically valid drone formations and interface cleanly with existing professional tooling. While limited in scope, the prototype demonstrates a practical method for linking generative visual concepts to executable formation data, and provides a foundation for future extensions into 3D sampling, temporal planning, or deeper integration with language-driven interfaces.

