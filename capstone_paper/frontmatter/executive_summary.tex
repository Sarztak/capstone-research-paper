\newpage
\section{Executive Summary}

Designing drone light shows is a complex process that combines creative choreography with technical precision. While commercial software platforms exist to support production, they are typically built for trained professionals and require manual specification of shapes, transitions, and safety constraints. As large language models (LLMs) become increasingly capable of interpreting natural language, researchers have proposed using them to translate high-level user prompts into drone swarm behaviors. However, current approaches typically use LLMs as translators — not as co-creative agents — and rarely engage with the realities of professional drone show design.

This project investigates whether language-based agents can meaningfully assist in the early stages of drone choreography, with a focus on feasibility, interpretability, and user alignment. Our goal is not to automate the design process, but to explore how LLMs can serve as collaborative tools that reduce the cognitive and operational overhead of translating intent into executable formations.

We begin by reviewing both academic papers and commercial tools. Prior work
includes systems such as CLIPSwarm \parencite{pueyo2024clipswarm}, SwarmGPT-Primitive \parencite{jiao2023swarmgpt}, \\
Swarm-GPT \parencite{vyas2024swarmgptprimitive}, FlockGPT \parencite{lykov2024flockgpt}, and LLM-Flock \parencite{li2025llmflock}. These systems demonstrate various architectures — from prompt-to-waypoint translation to decentralized plan generation — but none are grounded in actual design workflows or tested for usability by real users.

Commercially, tools like SPH Engineering's Drone Show Software and Verge Aero's Design Studio support professional use but lack intuitive or assistive interfaces for creative ideation. Our project situates itself in this gap: exploring how a prompt-based “design agent” could support iterative, interpretable, and rapid prototyping of drone choreographies.

To investigate this, we will:
\begin{enumerate}
    \item Interview professionals (if possible) to understand current workflows and bottlenecks.
    \item Develop a modular prototype agent that accepts user prompts, interprets them via LLMs, and visualizes resulting formations.
    \item Evaluate outputs based on interpretability, consistency, and alignment with user intent.
    \item Compare structured prompt templates against open-ended input to assess design trade-offs.  
\end{enumerate}

Our expected outcome is not a production-ready deployment tool, but a critically evaluated prototype and a set of design insights. These may inform future tools that aim to bridge the divide between intuitive expression and technical execution in drone choreography — whether through language models or other interfaces.

